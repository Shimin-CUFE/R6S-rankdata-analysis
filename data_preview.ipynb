{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "rankdata_000 = pd.read_csv(\n",
    "    r'D:\\Study\\Python\\Workspace\\rainbowSixSiege_analysis\\datadump_s5_ranked_data\\datadump_s5-000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 初步筛选\n",
    "data = rankdata_000.loc[:,\n",
    "       ['matchid', 'roundnumber', 'gamemode', 'mapname', 'roundduration', 'skillrank', 'role', 'haswon']]\n",
    "data = data.loc[data['role'] == 'Defender']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "              newmatchid gamemode     mapname  roundduration skillrank  \\\n0  1522380841_1_Defender  HOSTAGE  CLUB_HOUSE            124      Gold   \n1  1522380841_4_Defender  HOSTAGE  CLUB_HOUSE            217      Gold   \n2  1522380841_3_Defender  HOSTAGE  CLUB_HOUSE            160      Gold   \n3  1522380841_4_Defender  HOSTAGE  CLUB_HOUSE            217      Gold   \n4  1522380841_6_Defender  HOSTAGE  CLUB_HOUSE            143      Gold   \n\n   haswon  OP_BOPE-CAPITAO  OP_BOPE-CAVEIRA  OP_G.E.O.-JACKAL  OP_G.E.O.-MIRA  \\\n0       1                0                0                 0               0   \n1       1                0                0                 0               0   \n2       1                0                0                 0               0   \n3       1                0                1                 0               0   \n4       0                0                0                 0               0   \n\n   ...  OP_SPETSNAZ-FUZE  OP_SPETSNAZ-GLAZ  OP_SPETSNAZ-KAPKAN  \\\n0  ...                 0                 0                   0   \n1  ...                 0                 0                   0   \n2  ...                 0                 0                   0   \n3  ...                 0                 0                   0   \n4  ...                 0                 0                   0   \n\n   OP_SPETSNAZ-RESERVE  OP_SPETSNAZ-TACHANKA  OP_SWAT-ASH  OP_SWAT-CASTLE  \\\n0                    0                     0            0               1   \n1                    0                     0            0               0   \n2                    0                     0            0               0   \n3                    0                     0            0               0   \n4                    0                     0            0               0   \n\n   OP_SWAT-PULSE  OP_SWAT-RESERVE  OP_SWAT-THERMITE  \n0              0                0                 0  \n1              0                0                 0  \n2              0                0                 0  \n3              0                0                 0  \n4              0                0                 0  \n\n[5 rows x 41 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>newmatchid</th>\n      <th>gamemode</th>\n      <th>mapname</th>\n      <th>roundduration</th>\n      <th>skillrank</th>\n      <th>haswon</th>\n      <th>OP_BOPE-CAPITAO</th>\n      <th>OP_BOPE-CAVEIRA</th>\n      <th>OP_G.E.O.-JACKAL</th>\n      <th>OP_G.E.O.-MIRA</th>\n      <th>...</th>\n      <th>OP_SPETSNAZ-FUZE</th>\n      <th>OP_SPETSNAZ-GLAZ</th>\n      <th>OP_SPETSNAZ-KAPKAN</th>\n      <th>OP_SPETSNAZ-RESERVE</th>\n      <th>OP_SPETSNAZ-TACHANKA</th>\n      <th>OP_SWAT-ASH</th>\n      <th>OP_SWAT-CASTLE</th>\n      <th>OP_SWAT-PULSE</th>\n      <th>OP_SWAT-RESERVE</th>\n      <th>OP_SWAT-THERMITE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1522380841_1_Defender</td>\n      <td>HOSTAGE</td>\n      <td>CLUB_HOUSE</td>\n      <td>124</td>\n      <td>Gold</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1522380841_4_Defender</td>\n      <td>HOSTAGE</td>\n      <td>CLUB_HOUSE</td>\n      <td>217</td>\n      <td>Gold</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1522380841_3_Defender</td>\n      <td>HOSTAGE</td>\n      <td>CLUB_HOUSE</td>\n      <td>160</td>\n      <td>Gold</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1522380841_4_Defender</td>\n      <td>HOSTAGE</td>\n      <td>CLUB_HOUSE</td>\n      <td>217</td>\n      <td>Gold</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1522380841_6_Defender</td>\n      <td>HOSTAGE</td>\n      <td>CLUB_HOUSE</td>\n      <td>143</td>\n      <td>Gold</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 41 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 干员转换哑变量\n",
    "opdata = pd.get_dummies(rankdata_000['operator'], drop_first=False, prefix='OP')\n",
    "newdata = pd.merge(data, opdata, left_index=True, right_index=True)\n",
    "new_matchid = newdata['matchid'].map(str) + \"_\" + newdata['roundnumber'].map(str) + \"_\" + newdata['role'].map(str)\n",
    "newdata = newdata.drop(labels=['matchid', 'roundnumber', 'role'], axis=1)\n",
    "newdata.insert(0, 'newmatchid', new_matchid, allow_duplicates=False)\n",
    "newdata.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# 数据数值化、离散化\n",
    "newdata['gamemode'] = newdata['gamemode'].replace({'HOSTAGE': 1, 'BOMB': 2, 'SECURE_AREA': 3})\n",
    "newdata['mapname'] = newdata['mapname'].replace(\n",
    "    {'CLUB_HOUSE': 1, 'PLANE': 2, 'KANAL': 3, 'HEREFORD_BASE': 4, 'CONSULATE': 5,\n",
    "     'YACHT': 6, 'OREGON': 7, 'BORDER': 8, 'SKYSCRAPER': 9, 'BANK': 10, 'COASTLINE': 11,\n",
    "     'BARTLETT_U.': 12, 'HOUSE': 13, 'KAFE_DOSTOYEVSKY': 14, 'FAVELAS': 15, 'CHALET': 16})\n",
    "newdata['skillrank'] = newdata['skillrank'].replace(\n",
    "    {'Gold': 4, 'Unranked': 0, 'Platinum': 5, 'Silver': 3, 'Bronze': 1, 'Copper': 2, 'Diamond': 4})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2003883, 41)\n",
      "(276252, 41)\n"
     ]
    }
   ],
   "source": [
    "# 个人数据转为对局数据\n",
    "# 利用对局时间筛选无效数据\n",
    "print(newdata.shape)\n",
    "newdata_cp = newdata.copy()\n",
    "newdata_cp = newdata_cp.loc[(newdata_cp['roundduration'] >= 90) & (newdata_cp['roundduration'] <= 210)]\n",
    "res = newdata_cp.groupby(newdata_cp['newmatchid']).agg({max}).reset_index()\n",
    "print(res.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(276252, 1)\n",
      "(276252, 35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Think\\.conda\\envs\\pytorch_gpu\\lib\\site-packages\\pandas\\core\\generic.py:4150: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# 转换为numpy训练矩阵\n",
    "SCALE = res.shape[0]\n",
    "label = np.array(res.loc[:SCALE - 1, :]['haswon'])\n",
    "print(label.shape)\n",
    "train_data = np.array(\n",
    "    res.drop(labels=['newmatchid', 'haswon', 'gamemode', 'mapname', 'roundduration', 'skillrank'], axis=1).loc[:SCALE - 1, :])\n",
    "print(train_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# # SVM with sklearn\n",
    "# from matplotlib import pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn import svm, metrics\n",
    "#\n",
    "# x_train, x_test, y_train, y_test = train_test_split(train_data, label, random_state=1, train_size=0.6)\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print(x_test.shape)\n",
    "# print(y_test.shape)\n",
    "#\n",
    "# clf = svm.SVC(kernel='linear', decision_function_shape='ovo', probability=True)\n",
    "# clf.fit(x_train, y_train)\n",
    "# clf.score(x_train, y_train)\n",
    "# y_pre = clf.predict(x_test)\n",
    "# svm_predprob = clf.predict_proba(x_test)[:, 1]\n",
    "# print(\"[SVM]AUC Score (Train): %f\" % metrics.roc_auc_score(y_test, svm_predprob, multi_class='ovo'))\n",
    "#\n",
    "# fpr1, tpr1, thresholds1 = metrics.roc_curve(y_test, y_pre)\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.xlim(0, 1)\n",
    "# plt.ylim(0.0, 1.1)\n",
    "# plt.plot(fpr1, tpr1, color='red')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=35, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=16, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=16, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.714327  [    0/165751]\n",
      "loss: 0.716793  [10240/165751]\n",
      "loss: 0.708411  [20480/165751]\n",
      "loss: 0.712518  [30720/165751]\n",
      "loss: 0.702574  [40960/165751]\n",
      "loss: 0.704632  [51200/165751]\n",
      "loss: 0.707318  [61440/165751]\n",
      "loss: 0.705574  [71680/165751]\n",
      "loss: 0.700110  [81920/165751]\n",
      "loss: 0.699802  [92160/165751]\n",
      "loss: 0.696229  [102400/165751]\n",
      "loss: 0.694798  [112640/165751]\n",
      "loss: 0.692060  [122880/165751]\n",
      "loss: 0.688538  [133120/165751]\n",
      "loss: 0.686390  [143360/165751]\n",
      "loss: 0.684431  [153600/165751]\n",
      "loss: 0.682683  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.683080 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.685066  [    0/165751]\n",
      "loss: 0.680147  [10240/165751]\n",
      "loss: 0.680221  [20480/165751]\n",
      "loss: 0.679484  [30720/165751]\n",
      "loss: 0.678886  [40960/165751]\n",
      "loss: 0.676856  [51200/165751]\n",
      "loss: 0.686980  [61440/165751]\n",
      "loss: 0.685986  [71680/165751]\n",
      "loss: 0.674005  [81920/165751]\n",
      "loss: 0.669254  [92160/165751]\n",
      "loss: 0.674533  [102400/165751]\n",
      "loss: 0.679188  [112640/165751]\n",
      "loss: 0.678755  [122880/165751]\n",
      "loss: 0.679638  [133120/165751]\n",
      "loss: 0.674650  [143360/165751]\n",
      "loss: 0.677110  [153600/165751]\n",
      "loss: 0.663367  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.675586 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.673967  [    0/165751]\n",
      "loss: 0.669543  [10240/165751]\n",
      "loss: 0.677059  [20480/165751]\n",
      "loss: 0.671935  [30720/165751]\n",
      "loss: 0.682787  [40960/165751]\n",
      "loss: 0.675099  [51200/165751]\n",
      "loss: 0.668778  [61440/165751]\n",
      "loss: 0.676091  [71680/165751]\n",
      "loss: 0.676846  [81920/165751]\n",
      "loss: 0.666690  [92160/165751]\n",
      "loss: 0.681224  [102400/165751]\n",
      "loss: 0.667954  [112640/165751]\n",
      "loss: 0.668856  [122880/165751]\n",
      "loss: 0.679921  [133120/165751]\n",
      "loss: 0.677053  [143360/165751]\n",
      "loss: 0.680667  [153600/165751]\n",
      "loss: 0.676055  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.674436 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.678974  [    0/165751]\n",
      "loss: 0.678427  [10240/165751]\n",
      "loss: 0.674225  [20480/165751]\n",
      "loss: 0.669899  [30720/165751]\n",
      "loss: 0.678163  [40960/165751]\n",
      "loss: 0.666986  [51200/165751]\n",
      "loss: 0.668887  [61440/165751]\n",
      "loss: 0.675305  [71680/165751]\n",
      "loss: 0.683384  [81920/165751]\n",
      "loss: 0.680405  [92160/165751]\n",
      "loss: 0.680804  [102400/165751]\n",
      "loss: 0.666078  [112640/165751]\n",
      "loss: 0.672640  [122880/165751]\n",
      "loss: 0.672306  [133120/165751]\n",
      "loss: 0.671732  [143360/165751]\n",
      "loss: 0.678270  [153600/165751]\n",
      "loss: 0.671573  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.673590 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.674916  [    0/165751]\n",
      "loss: 0.675640  [10240/165751]\n",
      "loss: 0.670235  [20480/165751]\n",
      "loss: 0.675232  [30720/165751]\n",
      "loss: 0.682302  [40960/165751]\n",
      "loss: 0.667471  [51200/165751]\n",
      "loss: 0.672358  [61440/165751]\n",
      "loss: 0.677515  [71680/165751]\n",
      "loss: 0.677274  [81920/165751]\n",
      "loss: 0.664014  [92160/165751]\n",
      "loss: 0.662178  [102400/165751]\n",
      "loss: 0.668798  [112640/165751]\n",
      "loss: 0.670099  [122880/165751]\n",
      "loss: 0.666026  [133120/165751]\n",
      "loss: 0.676836  [143360/165751]\n",
      "loss: 0.665013  [153600/165751]\n",
      "loss: 0.670936  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.672876 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.675888  [    0/165751]\n",
      "loss: 0.671305  [10240/165751]\n",
      "loss: 0.670593  [20480/165751]\n",
      "loss: 0.673370  [30720/165751]\n",
      "loss: 0.682976  [40960/165751]\n",
      "loss: 0.673313  [51200/165751]\n",
      "loss: 0.666389  [61440/165751]\n",
      "loss: 0.668203  [71680/165751]\n",
      "loss: 0.668897  [81920/165751]\n",
      "loss: 0.676685  [92160/165751]\n",
      "loss: 0.665530  [102400/165751]\n",
      "loss: 0.677949  [112640/165751]\n",
      "loss: 0.667753  [122880/165751]\n",
      "loss: 0.670886  [133120/165751]\n",
      "loss: 0.685040  [143360/165751]\n",
      "loss: 0.674897  [153600/165751]\n",
      "loss: 0.667300  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.672281 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.671952  [    0/165751]\n",
      "loss: 0.672104  [10240/165751]\n",
      "loss: 0.673718  [20480/165751]\n",
      "loss: 0.670529  [30720/165751]\n",
      "loss: 0.680405  [40960/165751]\n",
      "loss: 0.676865  [51200/165751]\n",
      "loss: 0.669075  [61440/165751]\n",
      "loss: 0.659170  [71680/165751]\n",
      "loss: 0.676614  [81920/165751]\n",
      "loss: 0.665782  [92160/165751]\n",
      "loss: 0.664792  [102400/165751]\n",
      "loss: 0.658243  [112640/165751]\n",
      "loss: 0.663329  [122880/165751]\n",
      "loss: 0.679801  [133120/165751]\n",
      "loss: 0.679394  [143360/165751]\n",
      "loss: 0.685621  [153600/165751]\n",
      "loss: 0.672851  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.671681 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.671836  [    0/165751]\n",
      "loss: 0.675974  [10240/165751]\n",
      "loss: 0.671948  [20480/165751]\n",
      "loss: 0.676862  [30720/165751]\n",
      "loss: 0.674674  [40960/165751]\n",
      "loss: 0.672170  [51200/165751]\n",
      "loss: 0.659033  [61440/165751]\n",
      "loss: 0.680466  [71680/165751]\n",
      "loss: 0.672024  [81920/165751]\n",
      "loss: 0.672975  [92160/165751]\n",
      "loss: 0.671623  [102400/165751]\n",
      "loss: 0.678882  [112640/165751]\n",
      "loss: 0.679332  [122880/165751]\n",
      "loss: 0.676830  [133120/165751]\n",
      "loss: 0.668991  [143360/165751]\n",
      "loss: 0.680789  [153600/165751]\n",
      "loss: 0.667738  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.671045 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.672889  [    0/165751]\n",
      "loss: 0.671269  [10240/165751]\n",
      "loss: 0.662539  [20480/165751]\n",
      "loss: 0.669078  [30720/165751]\n",
      "loss: 0.684385  [40960/165751]\n",
      "loss: 0.669167  [51200/165751]\n",
      "loss: 0.682925  [61440/165751]\n",
      "loss: 0.666498  [71680/165751]\n",
      "loss: 0.671818  [81920/165751]\n",
      "loss: 0.667301  [92160/165751]\n",
      "loss: 0.653876  [102400/165751]\n",
      "loss: 0.667670  [112640/165751]\n",
      "loss: 0.670042  [122880/165751]\n",
      "loss: 0.663699  [133120/165751]\n",
      "loss: 0.679452  [143360/165751]\n",
      "loss: 0.665487  [153600/165751]\n",
      "loss: 0.664377  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.670423 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.673549  [    0/165751]\n",
      "loss: 0.672193  [10240/165751]\n",
      "loss: 0.658538  [20480/165751]\n",
      "loss: 0.685949  [30720/165751]\n",
      "loss: 0.666586  [40960/165751]\n",
      "loss: 0.667156  [51200/165751]\n",
      "loss: 0.670851  [61440/165751]\n",
      "loss: 0.664442  [71680/165751]\n",
      "loss: 0.666835  [81920/165751]\n",
      "loss: 0.671045  [92160/165751]\n",
      "loss: 0.668564  [102400/165751]\n",
      "loss: 0.669009  [112640/165751]\n",
      "loss: 0.668687  [122880/165751]\n",
      "loss: 0.667102  [133120/165751]\n",
      "loss: 0.670789  [143360/165751]\n",
      "loss: 0.659809  [153600/165751]\n",
      "loss: 0.669533  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.669789 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.663842  [    0/165751]\n",
      "loss: 0.674462  [10240/165751]\n",
      "loss: 0.663762  [20480/165751]\n",
      "loss: 0.669520  [30720/165751]\n",
      "loss: 0.669021  [40960/165751]\n",
      "loss: 0.672483  [51200/165751]\n",
      "loss: 0.681409  [61440/165751]\n",
      "loss: 0.657528  [71680/165751]\n",
      "loss: 0.670058  [81920/165751]\n",
      "loss: 0.673652  [92160/165751]\n",
      "loss: 0.671947  [102400/165751]\n",
      "loss: 0.679284  [112640/165751]\n",
      "loss: 0.666215  [122880/165751]\n",
      "loss: 0.671890  [133120/165751]\n",
      "loss: 0.678433  [143360/165751]\n",
      "loss: 0.671053  [153600/165751]\n",
      "loss: 0.666353  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.669132 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.665854  [    0/165751]\n",
      "loss: 0.663006  [10240/165751]\n",
      "loss: 0.668885  [20480/165751]\n",
      "loss: 0.665191  [30720/165751]\n",
      "loss: 0.659628  [40960/165751]\n",
      "loss: 0.660672  [51200/165751]\n",
      "loss: 0.668727  [61440/165751]\n",
      "loss: 0.678241  [71680/165751]\n",
      "loss: 0.673678  [81920/165751]\n",
      "loss: 0.672739  [92160/165751]\n",
      "loss: 0.666196  [102400/165751]\n",
      "loss: 0.670907  [112640/165751]\n",
      "loss: 0.667220  [122880/165751]\n",
      "loss: 0.679784  [133120/165751]\n",
      "loss: 0.671636  [143360/165751]\n",
      "loss: 0.665552  [153600/165751]\n",
      "loss: 0.675147  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.668367 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.676375  [    0/165751]\n",
      "loss: 0.676179  [10240/165751]\n",
      "loss: 0.664682  [20480/165751]\n",
      "loss: 0.666621  [30720/165751]\n",
      "loss: 0.659878  [40960/165751]\n",
      "loss: 0.655832  [51200/165751]\n",
      "loss: 0.675187  [61440/165751]\n",
      "loss: 0.674125  [71680/165751]\n",
      "loss: 0.672855  [81920/165751]\n",
      "loss: 0.667165  [92160/165751]\n",
      "loss: 0.664665  [102400/165751]\n",
      "loss: 0.662747  [112640/165751]\n",
      "loss: 0.661325  [122880/165751]\n",
      "loss: 0.671181  [133120/165751]\n",
      "loss: 0.668746  [143360/165751]\n",
      "loss: 0.656387  [153600/165751]\n",
      "loss: 0.663534  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.667589 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.669320  [    0/165751]\n",
      "loss: 0.672902  [10240/165751]\n",
      "loss: 0.661295  [20480/165751]\n",
      "loss: 0.662779  [30720/165751]\n",
      "loss: 0.669421  [40960/165751]\n",
      "loss: 0.671394  [51200/165751]\n",
      "loss: 0.663399  [61440/165751]\n",
      "loss: 0.666536  [71680/165751]\n",
      "loss: 0.659154  [81920/165751]\n",
      "loss: 0.664617  [92160/165751]\n",
      "loss: 0.678571  [102400/165751]\n",
      "loss: 0.668618  [112640/165751]\n",
      "loss: 0.675218  [122880/165751]\n",
      "loss: 0.667604  [133120/165751]\n",
      "loss: 0.678273  [143360/165751]\n",
      "loss: 0.666539  [153600/165751]\n",
      "loss: 0.663783  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.666768 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.661487  [    0/165751]\n",
      "loss: 0.663540  [10240/165751]\n",
      "loss: 0.671023  [20480/165751]\n",
      "loss: 0.663477  [30720/165751]\n",
      "loss: 0.663668  [40960/165751]\n",
      "loss: 0.677367  [51200/165751]\n",
      "loss: 0.662781  [61440/165751]\n",
      "loss: 0.656035  [71680/165751]\n",
      "loss: 0.661853  [81920/165751]\n",
      "loss: 0.664217  [92160/165751]\n",
      "loss: 0.664745  [102400/165751]\n",
      "loss: 0.667263  [112640/165751]\n",
      "loss: 0.677347  [122880/165751]\n",
      "loss: 0.674995  [133120/165751]\n",
      "loss: 0.661165  [143360/165751]\n",
      "loss: 0.663670  [153600/165751]\n",
      "loss: 0.655384  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.666031 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.658992  [    0/165751]\n",
      "loss: 0.667506  [10240/165751]\n",
      "loss: 0.659503  [20480/165751]\n",
      "loss: 0.665874  [30720/165751]\n",
      "loss: 0.664884  [40960/165751]\n",
      "loss: 0.656260  [51200/165751]\n",
      "loss: 0.674412  [61440/165751]\n",
      "loss: 0.667124  [71680/165751]\n",
      "loss: 0.657812  [81920/165751]\n",
      "loss: 0.668097  [92160/165751]\n",
      "loss: 0.665647  [102400/165751]\n",
      "loss: 0.667749  [112640/165751]\n",
      "loss: 0.669978  [122880/165751]\n",
      "loss: 0.671839  [133120/165751]\n",
      "loss: 0.665396  [143360/165751]\n",
      "loss: 0.666247  [153600/165751]\n",
      "loss: 0.658117  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.665254 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.673496  [    0/165751]\n",
      "loss: 0.665640  [10240/165751]\n",
      "loss: 0.656844  [20480/165751]\n",
      "loss: 0.667255  [30720/165751]\n",
      "loss: 0.660802  [40960/165751]\n",
      "loss: 0.663491  [51200/165751]\n",
      "loss: 0.644943  [61440/165751]\n",
      "loss: 0.659982  [71680/165751]\n",
      "loss: 0.667205  [81920/165751]\n",
      "loss: 0.667139  [92160/165751]\n",
      "loss: 0.662898  [102400/165751]\n",
      "loss: 0.650541  [112640/165751]\n",
      "loss: 0.671752  [122880/165751]\n",
      "loss: 0.663720  [133120/165751]\n",
      "loss: 0.671198  [143360/165751]\n",
      "loss: 0.666263  [153600/165751]\n",
      "loss: 0.669819  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.664490 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.666628  [    0/165751]\n",
      "loss: 0.665363  [10240/165751]\n",
      "loss: 0.665323  [20480/165751]\n",
      "loss: 0.670653  [30720/165751]\n",
      "loss: 0.675778  [40960/165751]\n",
      "loss: 0.654907  [51200/165751]\n",
      "loss: 0.658541  [61440/165751]\n",
      "loss: 0.666479  [71680/165751]\n",
      "loss: 0.666529  [81920/165751]\n",
      "loss: 0.660831  [92160/165751]\n",
      "loss: 0.658169  [102400/165751]\n",
      "loss: 0.660182  [112640/165751]\n",
      "loss: 0.653036  [122880/165751]\n",
      "loss: 0.657813  [133120/165751]\n",
      "loss: 0.656328  [143360/165751]\n",
      "loss: 0.667913  [153600/165751]\n",
      "loss: 0.662797  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.663994 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.656952  [    0/165751]\n",
      "loss: 0.661729  [10240/165751]\n",
      "loss: 0.674024  [20480/165751]\n",
      "loss: 0.654855  [30720/165751]\n",
      "loss: 0.670402  [40960/165751]\n",
      "loss: 0.662256  [51200/165751]\n",
      "loss: 0.677313  [61440/165751]\n",
      "loss: 0.665353  [71680/165751]\n",
      "loss: 0.652900  [81920/165751]\n",
      "loss: 0.661633  [92160/165751]\n",
      "loss: 0.658986  [102400/165751]\n",
      "loss: 0.659453  [112640/165751]\n",
      "loss: 0.671026  [122880/165751]\n",
      "loss: 0.661788  [133120/165751]\n",
      "loss: 0.671571  [143360/165751]\n",
      "loss: 0.668369  [153600/165751]\n",
      "loss: 0.665923  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.663282 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.670106  [    0/165751]\n",
      "loss: 0.645602  [10240/165751]\n",
      "loss: 0.659507  [20480/165751]\n",
      "loss: 0.661328  [30720/165751]\n",
      "loss: 0.659097  [40960/165751]\n",
      "loss: 0.660064  [51200/165751]\n",
      "loss: 0.668801  [61440/165751]\n",
      "loss: 0.663429  [71680/165751]\n",
      "loss: 0.651588  [81920/165751]\n",
      "loss: 0.661064  [92160/165751]\n",
      "loss: 0.682672  [102400/165751]\n",
      "loss: 0.667327  [112640/165751]\n",
      "loss: 0.662114  [122880/165751]\n",
      "loss: 0.660478  [133120/165751]\n",
      "loss: 0.657370  [143360/165751]\n",
      "loss: 0.662222  [153600/165751]\n",
      "loss: 0.648464  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.662772 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.671877  [    0/165751]\n",
      "loss: 0.654497  [10240/165751]\n",
      "loss: 0.656842  [20480/165751]\n",
      "loss: 0.667981  [30720/165751]\n",
      "loss: 0.659710  [40960/165751]\n",
      "loss: 0.657977  [51200/165751]\n",
      "loss: 0.656131  [61440/165751]\n",
      "loss: 0.653455  [71680/165751]\n",
      "loss: 0.668459  [81920/165751]\n",
      "loss: 0.659344  [92160/165751]\n",
      "loss: 0.674402  [102400/165751]\n",
      "loss: 0.663198  [112640/165751]\n",
      "loss: 0.662658  [122880/165751]\n",
      "loss: 0.657807  [133120/165751]\n",
      "loss: 0.663627  [143360/165751]\n",
      "loss: 0.649838  [153600/165751]\n",
      "loss: 0.655505  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.662402 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.663208  [    0/165751]\n",
      "loss: 0.671288  [10240/165751]\n",
      "loss: 0.662827  [20480/165751]\n",
      "loss: 0.657477  [30720/165751]\n",
      "loss: 0.664117  [40960/165751]\n",
      "loss: 0.661489  [51200/165751]\n",
      "loss: 0.663119  [61440/165751]\n",
      "loss: 0.675549  [71680/165751]\n",
      "loss: 0.651870  [81920/165751]\n",
      "loss: 0.670915  [92160/165751]\n",
      "loss: 0.660673  [102400/165751]\n",
      "loss: 0.655143  [112640/165751]\n",
      "loss: 0.653537  [122880/165751]\n",
      "loss: 0.656662  [133120/165751]\n",
      "loss: 0.659937  [143360/165751]\n",
      "loss: 0.665961  [153600/165751]\n",
      "loss: 0.665812  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.662029 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.661090  [    0/165751]\n",
      "loss: 0.656440  [10240/165751]\n",
      "loss: 0.661418  [20480/165751]\n",
      "loss: 0.665230  [30720/165751]\n",
      "loss: 0.666128  [40960/165751]\n",
      "loss: 0.664855  [51200/165751]\n",
      "loss: 0.663558  [61440/165751]\n",
      "loss: 0.652079  [71680/165751]\n",
      "loss: 0.655995  [81920/165751]\n",
      "loss: 0.669741  [92160/165751]\n",
      "loss: 0.669917  [102400/165751]\n",
      "loss: 0.658952  [112640/165751]\n",
      "loss: 0.669783  [122880/165751]\n",
      "loss: 0.659616  [133120/165751]\n",
      "loss: 0.664137  [143360/165751]\n",
      "loss: 0.653046  [153600/165751]\n",
      "loss: 0.666086  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.661788 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.658264  [    0/165751]\n",
      "loss: 0.662388  [10240/165751]\n",
      "loss: 0.677871  [20480/165751]\n",
      "loss: 0.654952  [30720/165751]\n",
      "loss: 0.669643  [40960/165751]\n",
      "loss: 0.666104  [51200/165751]\n",
      "loss: 0.649434  [61440/165751]\n",
      "loss: 0.666473  [71680/165751]\n",
      "loss: 0.655333  [81920/165751]\n",
      "loss: 0.656693  [92160/165751]\n",
      "loss: 0.653378  [102400/165751]\n",
      "loss: 0.674028  [112640/165751]\n",
      "loss: 0.653069  [122880/165751]\n",
      "loss: 0.676539  [133120/165751]\n",
      "loss: 0.671703  [143360/165751]\n",
      "loss: 0.660924  [153600/165751]\n",
      "loss: 0.669172  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.661598 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.665597  [    0/165751]\n",
      "loss: 0.665284  [10240/165751]\n",
      "loss: 0.655097  [20480/165751]\n",
      "loss: 0.662967  [30720/165751]\n",
      "loss: 0.663872  [40960/165751]\n",
      "loss: 0.678195  [51200/165751]\n",
      "loss: 0.661319  [61440/165751]\n",
      "loss: 0.660311  [71680/165751]\n",
      "loss: 0.668983  [81920/165751]\n",
      "loss: 0.650242  [92160/165751]\n",
      "loss: 0.663348  [102400/165751]\n",
      "loss: 0.663811  [112640/165751]\n",
      "loss: 0.664837  [122880/165751]\n",
      "loss: 0.651206  [133120/165751]\n",
      "loss: 0.653055  [143360/165751]\n",
      "loss: 0.662311  [153600/165751]\n",
      "loss: 0.664842  [163840/165751]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.661465 \n",
      "\n",
      "Done! Running time: 68.05867028236389\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAhj0lEQVR4nO3deXxV1b338c8vMyEDZABCQggBIsSJIaLIqNUreq1Db+sVrWML9ar3sdbaam/bex+f63P7VKu1lnotDlhbZxG0rQMoKEQRwiQQpkAYEoaEMIVA5vX8cQ70XBokSJKdk/19v1555Zx19tn8ludlvmevtfbe5pxDRET8J8LrAkRExBsKABERn1IAiIj4lAJARMSnFAAiIj4V5XUBpyItLc3l5OR4XYaISFhZunTpHudc+vHtYRUAOTk5FBUVeV2GiEhYMbOtLbVrCEhExKcUACIiPqUAEBHxKQWAiIhPKQBERHxKASAi4lMKABERn/JFAMxeUc4fF7W4DFZExLd8EQDvrtrFswtLvS5DRKRT8UUADM1IYktVDTV1jV6XIiLSafgkABJxDtbtqva6FBGRTsMXAZDfNwmAtTsPelyJiEjn4YsAyOzRjaS4KIoVACIix/giAMyMoRlJOgIQEQnhiwCAwETwup3VNDU7r0sREekUfBMA+RlJHGloYmtVjdeliIh0Cv4JgGMTwVoJJCICPgqAQb0SiIwwince8LoUEZFOwTcBEBcdyaD0BB0BiIgE+SYAIHBCmFYCiYgE+CwAkth5oJZ9NfVelyIi4jlfBYDOCBYR+RtfBcDQjEAA6IxgERGfBUBaQizpibEKABERfBYAEDghTCuBRER8GABDM5IoqaimvrHZ61JERDzluwDI75tEQ5OjpOKQ16WIiHjKfwGQkQhoJZCIiO8CICe1O7FREZoIFhHf810AREVGMKSPzggWEfFdAEBgIrh450Gc070BRMS/fBsA+w83sOtgrdeliIh4xpcBoEtCiIi0MgDMbJKZrTezEjN74ATbXGdmxWa2xsxeCmn/ZbBtrZn9xsws2D7SzFYF93msvSMM6RNYCVS8QwEgIv510gAws0hgGnA5kA9MNrP847YZDDwIjHHOnQl8P9h+ITAGOAc4CzgPmBB821PAFGBw8GfS6XendRLjoslOidcZwSLia605AhgFlDjnNjvn6oFXgKuP22YKMM05tw/AOVcRbHdAHBADxALRwG4zywCSnHOLXGAm9g/ANafbmVOhewOIiN+1JgAyge0hz8uCbaHygDwzKzSzRWY2CcA59xkwD9gZ/HnfObc2+P6yk+wTADObamZFZlZUWVnZmj61ytCMJEqrajhc39hm+xQRCSdtNQkcRWAYZyIwGZhuZj3MbBAwFMgi8Af+YjMbdyo7ds793jlX4JwrSE9Pb6NyAxeFcw7W7dIwkIj4U2sCoBzoF/I8K9gWqgx42znX4JwrBTYQCIRrgUXOuUPOuUPAu8Do4PuzTrLPdnXs3gCaCBYRn2pNACwBBpvZADOLAa4H3j5um1kEvv1jZmkEhoQ2A9uACWYWZWbRBCaA1zrndgIHzeyC4Oqfm4HZbdCfVsvq2Y3EuCjNA4iIb500AJxzjcDdwPvAWuA159waM3vIzK4KbvY+UGVmxQTG/O93zlUBbwCbgFXASmClc+6d4HvuBJ4BSoLbvNt23To5M2NoRpICQER8K6o1Gznn/gr89bi2n4c8dsAPgj+h2zQB3zvBPosILA31TH5GEq8Vbae52RER0WGnIYiIdAq+PBP4qPyMJA7XN7F172GvSxER6XC+DoCjE8EaBhIRP/J1AAzunUBkhGklkIj4kq8DIC46koHp3XUEICK+5OsAgL/dG0BExG98HwD5GUnsPFDL/sP1XpciItKhfB8Ax84I1lGAiPiMAkCXhBARn/J9AKQnxpKeGKt7A4iI7/g+AABdEkJEfEkBQODmMBsrqqlvbPa6FBGRDqMAILASqKHJsanykNeliIh0GAUAgQAATQSLiL8oAIABad2JjYrQPICI+IoCAIiKjOCMPoms3aUAEBH/UAAEDe2TRPGOgwRubSAi0vUpAILy+yax73ADuw/WeV2KiEiHUAAE6d4AIuI3CoCgIRmJgK4JJCL+oQAISoqLpl9KNwWAiPiGAiDE0D66JISI+IcCIER+3yRK99RwuL7R61JERNqdAiDE0IwknIP1u3RlUBHp+hQAIfJ1cxgR8REFQIisnt1IjI3SPICI+IICIISZBe8NoCEgEen6FADHye8bWAnU3KxLQohI16YAOM7QjEQO1zexbe9hr0sREWlXCoDj6JIQIuIXCoDj5PVOJDLCtBJIRLo8BcBx4qIjyU3rriMAEenyFAAtGJqRpNtDikiXpwBoQX7fJHYcqGX/4XqvSxERaTcKgBb8bSJY5wOISNelAGjB0UtCvL50O41NzR5XIyLSPhQALUhPjGXq+FxmLivnOy8UcbC2weuSRETanALgBH5yxVD+77VnU1iyh2unFbJlT43XJYmItKlWBYCZTTKz9WZWYmYPnGCb68ys2MzWmNlLwbaLzGxFyE+tmV0TfG2GmZWGvDasrTrVVm44P5sXv3M+VTX1XD2tkE9L9nhdkohImzlpAJhZJDANuBzIByabWf5x2wwGHgTGOOfOBL4P4Jyb55wb5pwbBlwMHAY+CHnr/Udfd86tOP3utL3RA1N5+66x9EqM5abnFvPioq1elyQi0iZacwQwCihxzm12ztUDrwBXH7fNFGCac24fgHOuooX9fBN41zkXdhfZyU6NZ+adFzIhL52fzVrNz2atpkGTwyIS5loTAJnA9pDnZcG2UHlAnpkVmtkiM5vUwn6uB14+ru1hM/vCzB43s9iW/nEzm2pmRWZWVFlZ2Ypy20diXDTTby7ge+NzeXHRVm55brHOExCRsNZWk8BRwGBgIjAZmG5mPY6+aGYZwNnA+yHveRAYApwHpAA/bmnHzrnfO+cKnHMF6enpbVTuVxMZYTx4xVAe/da5FG3ZxzXTCimp0LkCIhKeWhMA5UC/kOdZwbZQZcDbzrkG51wpsIFAIBx1HfCWc+7Yekrn3E4XUAc8T2CoKSx8c2QWL089n0N1jVw77VPmr29pxEtEpHNrTQAsAQab2QAziyEwlPP2cdvMIvDtHzNLIzAktDnk9ckcN/wTPCrAzAy4Blh9ytV7aGT/FGbfPZaslHhun7GEZxeW4pxuIiMi4eOkAeCcawTuJjB8sxZ4zTm3xsweMrOrgpu9D1SZWTEwj8DqnioAM8shcATx8XG7/pOZrQJWAWnAf7ZBfzpUZo9uvHHHaC7N783/+XMxP37zC2rqGr0uS0SkVSycvrUWFBS4oqIir8v4O83NjsfnbuDJj0romxzHz79+Jped2ZvAwY2IiLfMbKlzruD4dp0J3AYiIoz7/uEM3rhjNEndornjj0v5zgtFbKsKuxWvIuIjCoA2VJCTwjv/Opaf/uNQPt9cxaWPf8yTH26krrHJ69JERP6OAqCNRUdG8N1xucy9bwKXDO3Nr+Zs4PJfL2DhRl1GQkQ6FwVAO8lI7sa0G0fwwu2jaHaObz/7OXe/tIzdB2u9Lk1EBFAAtLsJeem89/3x3HtJHh8U7+Zrv/qY5xaW6j4DIuI5BUAHiIuO5J5LBvPB98czon9PHvpzMV//bSFLt+7zujQR8TEFQAfKSevOC7edx1M3jmBfTT3/9NSn/PD1lZTt02ohEel4UV4X4DdmxuVnZzAuL53ffLiRGYVbmL2inMmjsrnrokH0TorzukQR8QmdCOaxHfuP8Nt5Jby2ZDuREcbNo/tzx4SBpCa0eHFUEZFTdqITwRQAncS2qsM88eFG3lpeRlx0JLePGcCUcbkkx0d7XZqIhDkFQJgoqTjEEx9u5J2VO0iMi2LKuFxuG5NDYpyCQES+GgVAmFm78yCPz9nAB8W76RkfzR0TBnLz6By6xUR6XZqIhBkFQJhauX0/j83ZwMcbKklLiOXuiwZy/ahs4qIVBCLSOgqAMLdky14efX89n5fuJaV7DDeMyubbF/SnT7JWDYnIl1MAdAHOORZt3svzhaXMWbubCDMmndWH2y7MYWT/nrr8tIi06EQBoPMAwoiZMXpgKqMHprJ972FeXLSVVxZv4y9f7OSszCRuvXAAV56ToeEhEWkVHQGEucP1jby1vJwZhVvYWHGI1O4x3HB+Njeer+EhEQnQEFAX55zj001VPF+4hQ/X7Sby6PDQmBxGZGt4SMTPNATUxZkZYwalMWZQGtuqDvOHz7bwatF2/vzFTs7OTObWC3O48twMYqM0PCQiAToC6MJq6hqZubycGYWlbKqsIS0hlhvPz+bGC7LplajhIRG/0BCQjznnWLBxD88XljJvfSXRkcbXz+nLbWMGcHZWstfliUg70xCQj5kZ4/PSGZ+XTumeGl74dAuvF21n5vJyRvbvyW1jcph0Zh+iInV1cBE/0RGATx2sbeD1ojJe+HQL2/YeJiM5jptG92fyedn07B7jdXki0oY0BCQtamp2zFtXwfOfllJYUkVsVATXDs/ktjEDOKNPotfliUgbUADISa3fVc2MT0uZuaycusZmJp6RztTxuYzOTdUyUpEwpgCQVttXU88fF23lhc+2sOdQPedkJTN1fK7mCUTClAJATlltQxMzl5XzzILNbN5TQ7+Ubnx3bC7fKsgiPkbrB0TChQJAvrLmZsectbt5+uNNLNu2n57x0dw0OodbRvfXrStFwoACQNpE0Za9PP3JZuYU7yY2KoJvjsxiyrhcctK6e12aiJyAzgOQNlGQk0JBTgolFYd4ZsFmXi8q46XF25h0Zh+mjs9leHZPr0sUkVbSEYCclorqWl74dAsvfraVg7WNjOzfkynjBnBpfh8iI7RySKQz0BCQtKuaukZeL9rOs4WlbN97hP6p8dw+ZoAmjEU6AQWAdIimZscHa3YxfcFmlm3bT3K3aG48P5tbL8yhV5IuQCfiBQWAdLilW/fxzILNvL9mF5ERxlXnZvLdcQMYmpHkdWkivqJJYOlwI/v3ZGT/kWytquH5wi28VrSdN5eVMW5wGt8dl8v4wWk6w1jEQzoCkA5z4HADf1q8lRmFW6ioruOM3ol8d9wArh6WSUyUzjAWaS8aApJOo76xmXdW7mD6gs2s21VN76RYbhszgBvOzyYpLtrr8kS6nBMFQKu+dpnZJDNbb2YlZvbACba5zsyKzWyNmb0UbLvIzFaE/NSa2TXB1waY2efBfb5qZroGsU/EREXwTyOzePeecbxw+ygG9UrgF++u48L/+oiH/1LMzgNHvC5RxBdOegRgZpHABuBSoAxYAkx2zhWHbDMYeA242Dm3z8x6OecqjttPClACZDnnDpvZa8BM59wrZvbfwErn3FNfVouOALqu1eUH+P0nm/nLqp0YcNW5fZkyPlcTxiJt4HSOAEYBJc65zc65euAV4OrjtpkCTHPO7QM4/o9/0DeBd4N//A24GHgj+NoLwDWt6ol0SWdlJvObycOZ/8OJ3DS6P++t2cXlTyzg5ucWU1iyh3AaqhQJF60JgExge8jzsmBbqDwgz8wKzWyRmU1qYT/XAy8HH6cC+51zjV+yTwDMbKqZFZlZUWVlZSvKlXDWLyWef//6mXz2wNe4/7IzKN5xkBuf+Zwrn1zI7BXlNDY1e12iSJfRVksvooDBwERgMjDdzHocfdHMMoCzgfdPdcfOud875wqccwXp6eltU610esnx0dx10SAW/vgifvGNsznS0MQ9r6xgwiPz+cNnW6htaPK6RJGw15oAKAf6hTzPCraFKgPeds41OOdKCcwZDA55/TrgLedcQ/B5FdDDzI6eh9DSPkWIi47k+lHZzL13AtNvLqBPchw/n72Gsf/vI343v4SDtQ0n34mItKg1AbAEGBxctRNDYCjn7eO2mUXg2z9mlkZgSGhzyOuT+dvwDy4woDuPwLwAwC3A7FMvX/wiIsK4NL83b9wxmlenXkB+32R++d56xvzXR/zyvXXsOVTndYkiYadV5wGY2RXAr4FI4Dnn3MNm9hBQ5Jx7Ozip+ytgEtAEPOyceyX43hygEOjnnGsO2WcugQnlFGA58G3n3Jf+X6xVQBJqdfkBnpq/ib+u3klMZATXn9ePKeNzyeoZ73VpIp2KTgSTLmtT5SGe/ngTby0vxzm4elgm/zIxl0G9Er0uTaRTUABIl7dj/xGeWVDKy4u3UdvYxGX5fbjzooGck9XD69JEPKUAEN/YW1PPjMJSZny6hYO1jVw8pBcPXD6EvN46IhB/UgCI71TXNvDHRdt4an4Jh+oa+efz+nHvJXm6L4H4jgJAfGtfTT1PflTCi4u2EB0ZwZRxuUwdn0v3WF0NXfzhtC4GJxLOenaP4edfz2fuDyZw0ZBePPHhRiY+Op+XF2/TmcXiawoA8Y3+qd2ZdsMIZt55If1T4nlw5iouf2IBH63brWsNiS8pAMR3RmT35PU7RvPf3x5JY7Pj9hlF3DD9c1aXH/C6NJEOpQAQXzIzJp3Vhw/uHc9DV5/J+t3VXPnkQu59dQXl+3U/AvEHTQKLAAdrG/jv+Zt4dmEpDvjO2AHcOXEgibpDmXQBmgQW+RJJcdH8aNIQ5v1wIleek8FT8zcx8ZH5/HHRVk0US5elABAJ0bdHNx67bhjv3D2WQb0S+Oms1Vz+xALmra/QRLF0OQoAkRacnZXMK1Mv4OmbRtLQ1Mxtzy/h5ucWs27XQa9LE2kzCgCREzAzLjuzDx/cO4GfX5nPF2UHuOKJBTw48wsqqmu9Lk/ktCkARE4iJiqC28cO4OP7J3LbmAG8sbSMix6Zz28/2qg7k0lYUwCItFKP+Bh+dmU+H9w7gbGD03j0gw1c9Oh83lpeRnOz5gck/CgARE7RgLTuPH1TAa9OvYC0hFjufXUl1/6ukOXb9nldmsgpUQCIfEXn56Yy+64xPHbduew6WMu1v/uU+19fSWW1bk8p4UEBIHIaIiKMb4zI4sP7JvK9CbnMWlHOxY/O57mFpTp/QDo9BYBIG0iIjeLBy4fy3vfHMyy7Bw/9uZh//M1CPttU5XVpIiekABBpQwPTE/jD7aN4+qaR1NQ3Mnn6Iu5+aRk7D+j6QtL5KABE2tjR8wfm/mAC916Sx5zi3Vz86MdMm1dCXaOWjUrnoQAQaSdx0ZHcc8lg5v5gAuPz0njk/fVc9vgnzFtX4XVpIoACQKTd9UuJ5+mbCvjD7aOIiDBum7GE78xYQumeGq9LE59TAIh0kPF56bx3z3h+csUQFm2u4tLHPuans1ZRcVCXlRBv6H4AIh6oqK7ltx+V8NLn24iKNG4fM4DvTRhIcjfdf0Da3onuB6AAEPHQ1qoaHpuzgdkrdpDcLZp/mTiQW0bn0C0m0uvSpAtRAIh0Ymt2HODR99czb30lvZNiuedreXyrIIvoSI3SyunTHcFEOrEz+ybz/G2jeHXqBWT1jOcnb63i0sc+5p2VO3ShOWk3CgCRTuT83FTeuGM0z9xcQGxUJP/68nKumraQTzZU6o5k0uYUACKdjJlxSX5v/nrPOB7/53PZf7iBm59bzA3TP+eLsv1elyddiAJApJOKjDCuHZ7Fh/dN4H9fdSYbdldz1W8LuffVFezYr0tLyOnTJLBImKiubeCp+Zt4ZmEpBkwdn8v3JgwkITbK69Kkk9MksEiYS4yL5keThvDRfROYdFYfnvyohImPzOflxdto0kSxfAUKAJEwk9UznieuH85bd15I/9R4Hpy5in/8zQIWbKz0ujQJMwoAkTA1PLsnb9wxmt/dOIKa+kZuenYxtz6/mI27q70uTcKEAkAkjJkZV5ydwdwfTODfrhjK0q37mPTEAv7trVXsOaRbU8qXa1UAmNkkM1tvZiVm9sAJtrnOzIrNbI2ZvRTSnm1mH5jZ2uDrOcH2GWZWamYrgj/D2qJDIn4UGxXJlPG5fHz/RXz7/GxeWbKdiY/M53fzS6ht0D0IpGUnXQVkZpHABuBSoAxYAkx2zhWHbDMYeA242Dm3z8x6Oecqgq/NBx52zs0xswSg2Tl32MxmAH92zr3R2mK1CkikdTZVHuK//rqWuWsr6Jscxw8vO4NrhmUSEWFelyYeOJ1VQKOAEufcZudcPfAKcPVx20wBpjnn9gGE/PHPB6Kcc3OC7Yecc4dPox8i0goD0xN45pbzeHnKBaQmxPKD11Zy5ZMLWbhxj9elSSfSmgDIBLaHPC8LtoXKA/LMrNDMFpnZpJD2/WY208yWm9kjwSOKox42sy/M7HEzi/3KvRCRFo0emMrsu8bwxPXDOHCkgW8/+zm3PLeYdbsOel2adAJtNQkcBQwGJgKTgelm1iPYPg74IXAekAvcGnzPg8CQYHsK8OOWdmxmU82syMyKKiu1zE3kVEVEGFcPy+TD+ybwkyuGsHzbPq54YgE/emMluw7oZjR+1poAKAf6hTzPCraFKgPeds41OOdKCcwZDA62rwgOHzUCs4ARAM65nS6gDniewFDT33HO/d45V+CcK0hPTz+FrolIqLjoSKaOH8gnP7qI28cMYNbyHUx8dB6/+mA9h+oavS5PPNCaAFgCDDazAWYWA1wPvH3cNrMIfPvHzNIIDP1sDr63h5kd/ct9MVAc3C4j+NuAa4DVp9EPEWmlHvEx/PTKfD68bwKX5gfOKJ7wy3m8+NkWGpqavS5POtBJAyD4zf1u4H1gLfCac26NmT1kZlcFN3sfqDKzYmAecL9zrso510Rg+OdDM1sFGDA9+J4/BdtWAWnAf7Zlx0Tky/VLiefJycOZfdcYBvZK4Gez13DZ45/w5tIyanRE4Au6GJyI4Jzjw7UV/OK9dZRUHKJbdCSXndmbq4dnMm5QGlG6M1lYO9EyUF1GUESO3YPg4iG9WLptH28tL+cvX+xk1oodpCXEcOU5fblmeCbnZiUTGLWVrkBHACLSorrGJuavr2T2inLmrq2gvrGZAWnduXpYX64ZlklOWnevS5RW0k3hReQrO3CkgfdW72TW8h0sKq3CORie3YNrhmVy5TkZpCboNJ7OTAEgIm1ix/4jvL1yB7OWl7NuVzVREca1wzP5X18bTL+UeK/LkxYoAESkza3deZBXl2znpcXbcM5xXUE/7r54EBnJ3bwuTUIoAESk3ew8cITfflTCa0XbMTNuPD+bOycOIj1RQ0OdgQJARNrd9r2HefKjjby5rJyYyAhuvrA/d4wfSM/uMV6X5msKABHpMKV7anhi7gZmr9xB95gobh+Tw3fG5ZLcLdrr0nxJASAiHW7D7mp+PXcDf121i6S4KKaOz+XWMQNIiNUpSB1JASAinlmz4wCPz9nA3LUVpHSPYcq4XL5VkEWalo92CAWAiHhu+bZ9PDZnAws27iEywpiYl843RmTxtaG9iIuOPPkO5CtRAIhIp7FxdzVvLitn1vJydh2sJSkuiivP7cs/jchkRHZPXW6ijSkARKTTaWp2fLapipnLynh39S6ONDTRPzWebwzP4hsjMnViWRtRAIhIp1ZT18h7q3fx5rIyPtscuNzEqJwUvjEikyvOySApTiuIvioFgIiEjfL9R5i1vJw3l5WxubKG2KgIJuSlM25wGmMGpTEgrbuGiU6BAkBEwo5zji/KDjBzWRlz11ZQvv8IAJk9ujFmUCpjBgUCQauJvpwCQETCmnOOrVWHWViyh8KSPXy6qYoDRxoAGJqRxNhBqYwdnM6onBS6xWhFUSgFgIh0KU3NjtXlB44FQtGWfdQ3NRMTGcHI/j0ZGxwuOjszmcgIfw8XKQBEpEs7Ut/Eki17KSzZw4KNeyjeeRCA5G7RXDgwlbGD0xg7KI3+qf67kY0CQER8pepQHYWbqli4sZKFG/ew40AtAP1SujF2UGBC+cKBqfSI7/oXqlMAiIhvOeco3VPDwuDRwaJNVVTXNWIGZ2cmM3ZQGmMHpzGyf09io7re/IECQEQkqLGpmZVl+1mwMTB/sHzbfhqbHTFREQxKT+CMPonk9U5kSJ9E8vok0jc5LqyXnSoAREROoLq2gc8372XJlr2s313Nhl3Vx4aMABJjo8jrk8gZfRI5o/ffwiFc7nOgABAROQUHjjSwcXc163ZVsyH4e/2u6mNLTwHSE2MZlJ5A/9R4slPj6Z/S/djjznTm8okCQBflFhFpQXK3aApyUijISTnW5pyjsrruf4TC5spDzF27mz2H6v/H+1O6x5CdEk//1Hj6p8STndr92OO0hFgiOsHSVAWAiEgrmRm9kuLolRTH+Lz0//HaobpGtlUdZtveGrZUHWZr8PHSrft4Z+UOmkMGW6IijLSEWHolxZJ+9HdiHOmJsfQK/qQHf9pzUloBICLSBhJio8jvm0R+36S/e62+sZny/UfYWlXDtr2H2X2wloqDdVRU17HzQC0ryw5QVVNHSyPyPeKjSU+I5embRpKbntCmNSsARETaWUxUBAPSujMg7cQnoTU2NbO3pp6K6joqqmuprK47FhKV1XUktcP9lBUAIiKdQFRkxLHhJUjukH8zokP+FRER6XQUACIiPqUAEBHxKQWAiIhPKQBERHxKASAi4lMKABERn1IAiIj4VFhdDdTMKoGtQBqwx+NyvOTn/vu57+Dv/qvvX11/51z68Y1hFQBHmVlRS5c29Qs/99/PfQd/9199b/u+awhIRMSnFAAiIj4VrgHwe68L8Jif++/nvoO/+6++t7GwnAMQEZHTF65HACIicpoUACIiPhV2AWBmk8xsvZmVmNkDXtfTkcxsi5mtMrMVZlbkdT3tzcyeM7MKM1sd0pZiZnPMbGPwd08va2wvJ+j7f5hZefDzX2FmV3hZY3sxs35mNs/Mis1sjZndE2z3y2d/ov63+ecfVnMAZhYJbAAuBcqAJcBk51yxp4V1EDPbAhQ453xxMoyZjQcOAX9wzp0VbPslsNc594vgF4Cezrkfe1lnezhB3/8DOOSce9TL2tqbmWUAGc65ZWaWCCwFrgFuxR+f/Yn6fx1t/PmH2xHAKKDEObfZOVcPvAJc7XFN0k6cc58Ae49rvhp4Ifj4BQL/Y3Q5J+i7LzjndjrnlgUfVwNrgUz889mfqP9tLtwCIBPYHvK8jHb6D9NJOeADM1tqZlO9LsYjvZ1zO4OPdwG9vSzGA3eb2RfBIaIuOQQSysxygOHA5/jwsz+u/9DGn3+4BYDfjXXOjQAuB+4KDhP4lguMX4bPGObpewoYCAwDdgK/8rSadmZmCcCbwPedcwdDX/PDZ99C/9v88w+3ACgH+oU8zwq2+YJzrjz4uwJ4i8CQmN/sDo6RHh0rrfC4ng7jnNvtnGtyzjUD0+nCn7+ZRRP44/cn59zMYLNvPvuW+t8en3+4BcASYLCZDTCzGOB64G2Pa+oQZtY9OCGEmXUH/gFY/eXv6pLeBm4JPr4FmO1hLR3q6B+/oGvpop+/mRnwLLDWOfdYyEu++OxP1P/2+PzDahUQQHDp06+BSOA559zD3lbUMcwsl8C3foAo4KWu3nczexmYSOBSuLuBfwdmAa8B2QQuDX6dc67LTZaeoO8TCRz+O2AL8L2QMfEuw8zGAguAVUBzsPknBMbB/fDZn6j/k2njzz/sAkBERNpGuA0BiYhIG1EAiIj4lAJARMSnFAAiIj6lABAR8SkFgIiITykARER86v8DY2XXyxE1Z+QAAAAASUVORK5CYII=\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MLP with PyTorch\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# choose device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# prepare data\n",
    "c = np.column_stack((train_data, label))  # 将y添加到x的最后一列\n",
    "np.random.shuffle(c)\n",
    "shuffled_data = c[:, :-1]  # 乱序后的x\n",
    "shuffled_label = c[:, -1]  # 同等乱序后的y\n",
    "SIZE = int(SCALE * 0.6)\n",
    "train_x = shuffled_data[:SIZE]\n",
    "test_x = shuffled_data[SIZE:]\n",
    "y = []\n",
    "for n in range(label.shape[0]):\n",
    "    if label[n] == 0:\n",
    "        y.append((1, 0))\n",
    "    else:\n",
    "        y.append((1, 0))\n",
    "train_y = np.array(shuffled_label[:SIZE])\n",
    "test_y = np.array(shuffled_label[SIZE:])\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, train):\n",
    "        if train:\n",
    "            self.data = torch.from_numpy(train_x.astype('float32')).to(torch.device(device))\n",
    "            self.labels = torch.from_numpy(train_y).to(torch.device(device))\n",
    "        else:\n",
    "            self.data = torch.from_numpy(test_x.astype('float32')).to(torch.device(device))\n",
    "            self.labels = torch.from_numpy(test_y).to(torch.device(device))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d = self.data[idx]\n",
    "        l = self.labels[idx]\n",
    "        return d, l\n",
    "\n",
    "\n",
    "learning_rate = 1e-4\n",
    "batch_size = 1024\n",
    "epochs = 25\n",
    "input_dim = train_data.shape[1]\n",
    "output_dim = label.shape[1]\n",
    "\n",
    "train_dataset = CustomDataset(train=True)\n",
    "test_dataset = CustomDataset(train=False)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, output_dim + 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(torch.device(device))\n",
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "test = []\n",
    "start_time = time.time()\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loss = test_loop(test_dataloader, model, loss_fn)\n",
    "    test.append(test_loss)\n",
    "print(\"Done! Running time: \" + str(time.time() - start_time))\n",
    "plt.plot(np.array(list(range(1, epochs + 1))), np.array(test))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-840d4f25",
   "language": "python",
   "display_name": "PyCharm (rainbowSixSiege_analysis)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}